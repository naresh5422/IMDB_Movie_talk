{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check directory and change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cdot/PycharmProjects/IMDB_Movie_talk/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cdot/PycharmProjects/IMDB_Movie_talk'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cdot/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/cdot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv(\"IMDBDataset.csv\")\n",
    "df = imdb_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "0    One of the other reviewers has mentioned that ...\n",
      "1    A wonderful little production. <br /><br />The...\n",
      "2    I thought this was a wonderful way to spend ti...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "feature = df[\"review\"]\n",
    "label = df[\"sentiment\"]\n",
    "\n",
    "print(type(feature))\n",
    "print(feature[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "label = le.fit_transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  encoded_label\n",
       "0  One of the other reviewers has mentioned that ...  positive              1\n",
       "1  A wonderful little production. <br /><br />The...  positive              1\n",
       "2  I thought this was a wonderful way to spend ti...  positive              1"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"encoded_label\"] = label\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "spwd = []\n",
    "need = [\"not\",\"no\"]\n",
    "for w in stopwords.words('english'):\n",
    "    if w not in need:\n",
    "        spwd.append(w)\n",
    "\n",
    "print(len(spwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one review mention watch oz episod hook right exactli happen br br first thing struck oz brutal unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word br br call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christian italian irish scuffl death stare dodgi deal shadi agreement never far away br br would say main appeal show due fact goe show dare forget pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first episod ever saw struck nasti surreal say readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard sold nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side', 'wonder littl product br br film techniqu unassum old time bbc fashion give comfort sometim discomfort sens realism entir piec br br actor extrem well chosen michael sheen got polari voic pat truli see seamless edit guid refer william diari entri well worth watch terrificli written perform piec master product one great master comedi life br br realism realli come home littl thing fantasi guard rather use tradit dream techniqu remain solid disappear play knowledg sens particularli scene concern orton halliwel set particularli flat halliwel mural decor everi surfac terribl well done', 'thought wonder way spend time hot summer weekend sit air condit theater watch light heart comedi plot simplist dialogu witti charact likabl even well bread suspect serial killer may disappoint realiz match point risk addict thought proof woodi allen still fulli control style mani us grown love br br laugh one woodi comedi year dare say decad never impress scarlet johanson manag tone sexi imag jump right averag spirit young woman br br may crown jewel career wittier devil wear prada interest superman great comedi go see friend']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for row in df[\"review\"]:\n",
    "    review = re.sub('[^a-zA-Z]', \" \", row)\n",
    "    review = review.lower()\n",
    "    review = word_tokenize(review)\n",
    "    review = [ps.stem(w) for w in review if w not in stopwords.words('english')]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>encoded_label</th>\n",
       "      <th>stemmed_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>one review mention watch oz episod hook right ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>wonder littl product br br film techniqu unass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>basic famili littl boy jake think zombi closet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  encoded_label  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive              1   \n",
       "1  A wonderful little production. <br /><br />The...  positive              1   \n",
       "2  I thought this was a wonderful way to spend ti...  positive              1   \n",
       "3  Basically there's a family where a little boy ...  negative              0   \n",
       "\n",
       "                                      stemmed_corpus  \n",
       "0  one review mention watch oz episod hook right ...  \n",
       "1  wonder littl product br br film techniqu unass...  \n",
       "2  thought wonder way spend time hot summer weeke...  \n",
       "3  basic famili littl boy jake think zombi closet...  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"stemmed_corpus\"] = corpus\n",
    "stem_df = df.copy()\n",
    "stem_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizatoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Corpus with 3 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = []\n",
    "for row in feature[0:3]:\n",
    "    review = str(sent_tokenize(row))\n",
    "    # review = re.sub('[^a-zA-Z0-9]', \" \", sent)\n",
    "    review = word_tokenize(review)\n",
    "    # print(review)\n",
    "    # print()\n",
    "    review = [w.lower() for w in review if w.isalpha()]\n",
    "    # print(review)\n",
    "    # print()\n",
    "    review = [ps.stem(w) for w in review if w not in stopwords.words('english')]\n",
    "    # print(review)\n",
    "    # print()\n",
    "    review = \" \".join(review)\n",
    "    corpus1.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
      "one review mention watch oz episod hook right exactli happen br br first thing struck oz brutal unflinch scene violenc set right word go show faint heart timid show pull punch regard drug sex violenc hardcor classic use br br call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christian italian irish scuffl death stare dodgi deal shadi agreement never far br br would say main appeal show due fact goe show would dare forget pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first episod ever saw struck nasti surreal could say readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard sold nickel inmat kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side\n"
     ]
    }
   ],
   "source": [
    "print(feature[0])\n",
    "print(corpus1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "feature1 = corpus\n",
    "print(type(feature))\n",
    "print(type(feature1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': 142, 'review': 174, 'mention': 132, 'watch': 237, 'oz': 146, 'episod': 58, 'hook': 99, 'right': 175, 'exactli': 62, 'happen': 94, 'br': 15, 'first': 73, 'thing': 221, 'struck': 208, 'brutal': 17, 'unflinch': 231, 'scene': 181, 'violenc': 235, 'set': 189, 'word': 247, 'go': 85, 'show': 194, 'faint': 68, 'heart': 96, 'timid': 224, 'pull': 164, 'punch': 165, 'regard': 172, 'drug': 51, 'sex': 190, 'hardcor': 95, 'classic': 26, 'use': 233, 'call': 18, 'nicknam': 141, 'given': 83, 'oswald': 145, 'maximum': 130, 'secur': 185, 'state': 205, 'penitentari': 150, 'focus': 75, 'mainli': 123, 'emerald': 55, 'citi': 24, 'experiment': 64, 'section': 184, 'prison': 160, 'cell': 20, 'glass': 84, 'front': 78, 'face': 66, 'inward': 106, 'privaci': 161, 'high': 97, 'agenda': 3, 'em': 54, 'home': 98, 'mani': 126, 'aryan': 9, 'muslim': 137, 'gangsta': 80, 'latino': 116, 'christian': 23, 'italian': 108, 'irish': 107, 'scuffl': 182, 'death': 39, 'stare': 204, 'dodgi': 49, 'deal': 38, 'shadi': 192, 'agreement': 4, 'never': 139, 'far': 70, 'would': 249, 'say': 179, 'main': 122, 'appeal': 7, 'due': 52, 'fact': 67, 'goe': 86, 'dare': 36, 'forget': 76, 'pretti': 159, 'pictur': 152, 'paint': 147, 'mainstream': 124, 'audienc': 10, 'charm': 22, 'romanc': 177, 'mess': 133, 'around': 8, 'ever': 60, 'saw': 178, 'nasti': 138, 'surreal': 213, 'could': 33, 'readi': 167, 'develop': 42, 'tast': 215, 'got': 87, 'accustom': 0, 'level': 118, 'graphic': 88, 'injustic': 103, 'crook': 34, 'guard': 91, 'sold': 199, 'nickel': 140, 'inmat': 104, 'kill': 112, 'order': 143, 'get': 81, 'away': 12, 'well': 241, 'manner': 127, 'middl': 135, 'class': 25, 'turn': 229, 'bitch': 14, 'lack': 115, 'street': 207, 'skill': 198, 'experi': 63, 'may': 131, 'becom': 13, 'comfort': 29, 'uncomfort': 230, 'view': 234, 'that': 219, 'touch': 226, 'darker': 37, 'side': 195, 'wonder': 245, 'littl': 121, 'product': 162, 'film': 72, 'techniqu': 216, 'fashion': 71, 'give': 82, 'sometim': 201, 'discomfort': 48, 'sens': 187, 'realism': 168, 'entir': 56, 'piec': 153, 'actor': 1, 'extrem': 65, 'michael': 134, 'sheen': 193, 'polari': 157, 'voic': 236, 'pat': 149, 'truli': 228, 'see': 186, 'seamless': 183, 'edit': 53, 'guid': 92, 'refer': 171, 'william': 242, 'diari': 45, 'entri': 57, 'worth': 248, 'terrificli': 218, 'written': 250, 'perform': 151, 'master': 128, 'great': 89, 'comedi': 28, 'life': 119, 'realli': 170, 'come': 27, 'fantasi': 69, 'rather': 166, 'tradit': 227, 'remain': 173, 'solid': 200, 'disappear': 46, 'play': 154, 'knowledg': 114, 'particularli': 148, 'concern': 30, 'orton': 144, 'halliwel': 93, 'flat': 74, 'mural': 136, 'decor': 41, 'everi': 61, 'surfac': 212, 'terribl': 217, 'done': 50, 'thought': 222, 'way': 238, 'spend': 202, 'time': 223, 'hot': 100, 'summer': 210, 'weekend': 240, 'sit': 197, 'air': 5, 'condit': 31, 'theater': 220, 'plot': 155, 'simplist': 196, 'dialogu': 44, 'witti': 243, 'charact': 21, 'likabl': 120, 'even': 59, 'bread': 16, 'suspect': 214, 'serial': 188, 'killer': 113, 'disappoint': 47, 'realiz': 169, 'match': 129, 'point': 156, 'risk': 176, 'addict': 2, 'proof': 163, 'woodi': 246, 'allen': 6, 'still': 206, 'fulli': 79, 'control': 32, 'style': 209, 'us': 232, 'grown': 90, 'laugh': 117, 'year': 251, 'decad': 40, 'impress': 102, 'scarlet': 180, 'johanson': 110, 'manag': 125, 'tone': 225, 'sexi': 191, 'imag': 101, 'jump': 111, 'averag': 11, 'spirit': 203, 'young': 252, 'crown': 35, 'jewel': 109, 'career': 19, 'wittier': 244, 'devil': 43, 'wear': 239, 'prada': 158, 'interest': 105, 'superman': 211, 'friend': 77}\n"
     ]
    }
   ],
   "source": [
    "cv1 = CountVectorizer(binary = True)\n",
    "x1 = cv1.fit(corpus1)\n",
    "feature_name1 = cv1.get_feature_names_out()\n",
    "print(x1.vocabulary_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 0 1 1 0\n",
      "  1 1 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0\n",
      "  0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1\n",
      "  1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
      "  0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1\n",
      "  0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1\n",
      "  0 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0\n",
      "  0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1\n",
      "  1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      "  1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0\n",
      "  0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0\n",
      "  0]\n",
      " [0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 1\n",
      "  1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0\n",
      "  0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
      "  1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0\n",
      "  0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1\n",
      "  1]]\n"
     ]
    }
   ],
   "source": [
    "voc = cv1.transform(corpus1)\n",
    "vect = voc.toarray()\n",
    "print(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Main corpus for total records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True, max_features=100, min_df=10)\n",
    "cv.fit(corpus)\n",
    "vect = cv.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of Vocabulary is:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"The Length of Vocabulary is: \", len(cv.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of Vector is:  (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"The Shape of Vector is: \",vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 58,\n",
       " 'watch': 93,\n",
       " 'right': 68,\n",
       " 'br': 11,\n",
       " 'first': 28,\n",
       " 'thing': 83,\n",
       " 'scene': 71,\n",
       " 'set': 75,\n",
       " 'go': 32,\n",
       " 'show': 76,\n",
       " 'use': 91,\n",
       " 'mani': 50,\n",
       " 'never': 54,\n",
       " 'would': 98,\n",
       " 'say': 70,\n",
       " 'fact': 24,\n",
       " 'around': 5,\n",
       " 'ever': 22,\n",
       " 'got': 34,\n",
       " 'get': 30,\n",
       " 'well': 95,\n",
       " 'turn': 89,\n",
       " 'wonder': 96,\n",
       " 'littl': 41,\n",
       " 'film': 26,\n",
       " 'old': 57,\n",
       " 'time': 87,\n",
       " 'give': 31,\n",
       " 'actor': 1,\n",
       " 'see': 72,\n",
       " 'perform': 61,\n",
       " 'great': 35,\n",
       " 'life': 39,\n",
       " 'realli': 67,\n",
       " 'come': 14,\n",
       " 'play': 62,\n",
       " 'everi': 23,\n",
       " 'thought': 86,\n",
       " 'way': 94,\n",
       " 'plot': 63,\n",
       " 'charact': 13,\n",
       " 'even': 21,\n",
       " 'point': 64,\n",
       " 'still': 80,\n",
       " 'love': 46,\n",
       " 'year': 99,\n",
       " 'interest': 37,\n",
       " 'think': 84,\n",
       " 'movi': 52,\n",
       " 'make': 48,\n",
       " 'like': 40,\n",
       " 'real': 66,\n",
       " 'seem': 73,\n",
       " 'peopl': 60,\n",
       " 'director': 18,\n",
       " 'new': 55,\n",
       " 'anoth': 4,\n",
       " 'know': 38,\n",
       " 'look': 44,\n",
       " 'live': 42,\n",
       " 'best': 9,\n",
       " 'find': 27,\n",
       " 'act': 0,\n",
       " 'good': 33,\n",
       " 'direct': 17,\n",
       " 'cast': 12,\n",
       " 'work': 97,\n",
       " 'stori': 81,\n",
       " 'seen': 74,\n",
       " 'role': 69,\n",
       " 'believ': 8,\n",
       " 'back': 6,\n",
       " 'quit': 65,\n",
       " 'funni': 29,\n",
       " 'bad': 7,\n",
       " 'also': 3,\n",
       " 'star': 78,\n",
       " 'made': 47,\n",
       " 'end': 19,\n",
       " 'actual': 2,\n",
       " 'someth': 77,\n",
       " 'better': 10,\n",
       " 'tri': 88,\n",
       " 'much': 53,\n",
       " 'enjoy': 20,\n",
       " 'long': 43,\n",
       " 'feel': 25,\n",
       " 'start': 79,\n",
       " 'take': 82,\n",
       " 'minut': 51,\n",
       " 'man': 49,\n",
       " 'lot': 45,\n",
       " 'could': 15,\n",
       " 'want': 92,\n",
       " 'guy': 36,\n",
       " 'part': 59,\n",
       " 'though': 85,\n",
       " 'noth': 56,\n",
       " 'two': 90,\n",
       " 'day': 16}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(vect.toarray()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   ...  90  91  92  93  94  95  96  \\\n",
      "0   0   0   0   0   0   1   0   0   0   0  ...   0   1   0   1   0   1   0   \n",
      "1   0   1   0   0   0   0   0   0   0   0  ...   0   1   0   1   0   1   1   \n",
      "2   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   1   1   1   1   \n",
      "3   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   1   0   1   0   \n",
      "4   1   0   0   0   1   0   0   0   0   1  ...   0   0   0   1   1   0   0   \n",
      "\n",
      "   97  98  99  \n",
      "0   0   1   0  \n",
      "1   0   0   0  \n",
      "2   0   0   1  \n",
      "3   0   0   0  \n",
      "4   1   0   0  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "vector_values = pd.DataFrame(vect.toarray())\n",
    "print(vector_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'review', 'mention', 'watch', 'oz', 'episod', 'hook', 'right', 'exactli', 'happen', 'br', 'br', 'first', 'thing', 'struck', 'oz', 'brutal', 'unflinch', 'scene', 'violenc', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'heart', 'timid', 'show', 'pull', 'punch', 'regard', 'drug', 'sex', 'violenc', 'hardcor', 'classic', 'use', 'word', 'br', 'br', 'call', 'oz', 'nicknam', 'given', 'oswald', 'maximum', 'secur', 'state', 'penitentari', 'focus', 'mainli', 'emerald', 'citi', 'experiment', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inward', 'privaci', 'high', 'agenda', 'em', 'citi', 'home', 'mani', 'aryan', 'muslim', 'gangsta', 'latino', 'christian', 'italian', 'irish', 'scuffl', 'death', 'stare', 'dodgi', 'deal', 'shadi', 'agreement', 'never', 'far', 'away', 'br', 'br', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goe', 'show', 'dare', 'forget', 'pretti', 'pictur', 'paint', 'mainstream', 'audienc', 'forget', 'charm', 'forget', 'romanc', 'oz', 'mess', 'around', 'first', 'episod', 'ever', 'saw', 'struck', 'nasti', 'surreal', 'say', 'readi', 'watch', 'develop', 'tast', 'oz', 'got', 'accustom', 'high', 'level', 'graphic', 'violenc', 'violenc', 'injustic', 'crook', 'guard', 'sold', 'nickel', 'inmat', 'kill', 'order', 'get', 'away', 'well', 'manner', 'middl', 'class', 'inmat', 'turn', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experi', 'watch', 'oz', 'may', 'becom', 'comfort', 'uncomfort', 'view', 'that', 'get', 'touch', 'darker', 'side']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_text = [s.split() for s in stem_df[\"stemmed_corpus\"]]\n",
    "print(list_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=19329, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "cbow = Word2Vec(list_text,vector_size=100, min_count=10, sg = 0)\n",
    "print(cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'br': 0,\n",
       " 'movi': 1,\n",
       " 'film': 2,\n",
       " 'one': 3,\n",
       " 'like': 4,\n",
       " 'time': 5,\n",
       " 'good': 6,\n",
       " 'make': 7,\n",
       " 'charact': 8,\n",
       " 'see': 9,\n",
       " 'get': 10,\n",
       " 'watch': 11,\n",
       " 'even': 12,\n",
       " 'stori': 13,\n",
       " 'would': 14,\n",
       " 'realli': 15,\n",
       " 'well': 16,\n",
       " 'scene': 17,\n",
       " 'look': 18,\n",
       " 'show': 19,\n",
       " 'much': 20,\n",
       " 'end': 21,\n",
       " 'bad': 22,\n",
       " 'great': 23,\n",
       " 'peopl': 24,\n",
       " 'go': 25,\n",
       " 'love': 26,\n",
       " 'also': 27,\n",
       " 'first': 28,\n",
       " 'think': 29,\n",
       " 'act': 30,\n",
       " 'play': 31,\n",
       " 'way': 32,\n",
       " 'thing': 33,\n",
       " 'made': 34,\n",
       " 'could': 35,\n",
       " 'know': 36,\n",
       " 'say': 37,\n",
       " 'seem': 38,\n",
       " 'work': 39,\n",
       " 'plot': 40,\n",
       " 'actor': 41,\n",
       " 'two': 42,\n",
       " 'mani': 43,\n",
       " 'seen': 44,\n",
       " 'come': 45,\n",
       " 'year': 46,\n",
       " 'want': 47,\n",
       " 'take': 48,\n",
       " 'never': 49,\n",
       " 'life': 50,\n",
       " 'best': 51,\n",
       " 'tri': 52,\n",
       " 'littl': 53,\n",
       " 'ever': 54,\n",
       " 'man': 55,\n",
       " 'better': 56,\n",
       " 'give': 57,\n",
       " 'still': 58,\n",
       " 'find': 59,\n",
       " 'perform': 60,\n",
       " 'feel': 61,\n",
       " 'part': 62,\n",
       " 'use': 63,\n",
       " 'someth': 64,\n",
       " 'director': 65,\n",
       " 'actual': 66,\n",
       " 'back': 67,\n",
       " 'lot': 68,\n",
       " 'interest': 69,\n",
       " 'real': 70,\n",
       " 'guy': 71,\n",
       " 'old': 72,\n",
       " 'funni': 73,\n",
       " 'cast': 74,\n",
       " 'though': 75,\n",
       " 'live': 76,\n",
       " 'anoth': 77,\n",
       " 'music': 78,\n",
       " 'enjoy': 79,\n",
       " 'star': 80,\n",
       " 'noth': 81,\n",
       " 'role': 82,\n",
       " 'start': 83,\n",
       " 'new': 84,\n",
       " 'point': 85,\n",
       " 'set': 86,\n",
       " 'everi': 87,\n",
       " 'girl': 88,\n",
       " 'day': 89,\n",
       " 'believ': 90,\n",
       " 'world': 91,\n",
       " 'origin': 92,\n",
       " 'turn': 93,\n",
       " 'thought': 94,\n",
       " 'horror': 95,\n",
       " 'quit': 96,\n",
       " 'direct': 97,\n",
       " 'minut': 98,\n",
       " 'comedi': 99,\n",
       " 'kill': 100,\n",
       " 'us': 101,\n",
       " 'fact': 102,\n",
       " 'pretti': 103,\n",
       " 'effect': 104,\n",
       " 'got': 105,\n",
       " 'action': 106,\n",
       " 'happen': 107,\n",
       " 'around': 108,\n",
       " 'wonder': 109,\n",
       " 'long': 110,\n",
       " 'young': 111,\n",
       " 'right': 112,\n",
       " 'big': 113,\n",
       " 'howev': 114,\n",
       " 'enough': 115,\n",
       " 'line': 116,\n",
       " 'fan': 117,\n",
       " 'may': 118,\n",
       " 'friend': 119,\n",
       " 'need': 120,\n",
       " 'bit': 121,\n",
       " 'person': 122,\n",
       " 'seri': 123,\n",
       " 'script': 124,\n",
       " 'must': 125,\n",
       " 'beauti': 126,\n",
       " 'without': 127,\n",
       " 'famili': 128,\n",
       " 'saw': 129,\n",
       " 'alway': 130,\n",
       " 'reason': 131,\n",
       " 'kid': 132,\n",
       " 'put': 133,\n",
       " 'becom': 134,\n",
       " 'almost': 135,\n",
       " 'tell': 136,\n",
       " 'final': 137,\n",
       " 'done': 138,\n",
       " 'laugh': 139,\n",
       " 'complet': 140,\n",
       " 'last': 141,\n",
       " 'whole': 142,\n",
       " 'least': 143,\n",
       " 'sure': 144,\n",
       " 'shot': 145,\n",
       " 'far': 146,\n",
       " 'place': 147,\n",
       " 'kind': 148,\n",
       " 'expect': 149,\n",
       " 'differ': 150,\n",
       " 'name': 151,\n",
       " 'mean': 152,\n",
       " 'call': 153,\n",
       " 'might': 154,\n",
       " 'anyth': 155,\n",
       " 'sinc': 156,\n",
       " 'tv': 157,\n",
       " 'book': 158,\n",
       " 'let': 159,\n",
       " 'probabl': 160,\n",
       " 'screen': 161,\n",
       " 'begin': 162,\n",
       " 'entertain': 163,\n",
       " 'help': 164,\n",
       " 'away': 165,\n",
       " 'yet': 166,\n",
       " 'moment': 167,\n",
       " 'woman': 168,\n",
       " 'fun': 169,\n",
       " 'anyon': 170,\n",
       " 'run': 171,\n",
       " 'american': 172,\n",
       " 'lead': 173,\n",
       " 'worst': 174,\n",
       " 'read': 175,\n",
       " 'hard': 176,\n",
       " 'rather': 177,\n",
       " 'audienc': 178,\n",
       " 'idea': 179,\n",
       " 'found': 180,\n",
       " 'war': 181,\n",
       " 'dvd': 182,\n",
       " 'bore': 183,\n",
       " 'keep': 184,\n",
       " 'hope': 185,\n",
       " 'appear': 186,\n",
       " 'although': 187,\n",
       " 'especi': 188,\n",
       " 'episod': 189,\n",
       " 'cours': 190,\n",
       " 'everyth': 191,\n",
       " 'move': 192,\n",
       " 'sens': 193,\n",
       " 'job': 194,\n",
       " 'goe': 195,\n",
       " 'mayb': 196,\n",
       " 'mind': 197,\n",
       " 'three': 198,\n",
       " 'worth': 199,\n",
       " 'recommend': 200,\n",
       " 'sound': 201,\n",
       " 'main': 202,\n",
       " 'someon': 203,\n",
       " 'face': 204,\n",
       " 'product': 205,\n",
       " 'problem': 206,\n",
       " 'follow': 207,\n",
       " 'boy': 208,\n",
       " 'night': 209,\n",
       " 'money': 210,\n",
       " 'version': 211,\n",
       " 'true': 212,\n",
       " 'special': 213,\n",
       " 'rate': 214,\n",
       " 'everyon': 215,\n",
       " 'second': 216,\n",
       " 'nice': 217,\n",
       " 'togeth': 218,\n",
       " 'human': 219,\n",
       " 'excel': 220,\n",
       " 'leav': 221,\n",
       " 'view': 222,\n",
       " 'wast': 223,\n",
       " 'black': 224,\n",
       " 'high': 225,\n",
       " 'hand': 226,\n",
       " 'hous': 227,\n",
       " 'eye': 228,\n",
       " 'death': 229,\n",
       " 'instead': 230,\n",
       " 'talk': 231,\n",
       " 'hour': 232,\n",
       " 'half': 233,\n",
       " 'said': 234,\n",
       " 'understand': 235,\n",
       " 'classic': 236,\n",
       " 'review': 237,\n",
       " 'anim': 238,\n",
       " 'open': 239,\n",
       " 'later': 240,\n",
       " 'john': 241,\n",
       " 'miss': 242,\n",
       " 'care': 243,\n",
       " 'head': 244,\n",
       " 'short': 245,\n",
       " 'left': 246,\n",
       " 'die': 247,\n",
       " 'wife': 248,\n",
       " 'write': 249,\n",
       " 'murder': 250,\n",
       " 'chang': 251,\n",
       " 'budget': 252,\n",
       " 'gener': 253,\n",
       " 'rememb': 254,\n",
       " 'father': 255,\n",
       " 'total': 256,\n",
       " 'surpris': 257,\n",
       " 'els': 258,\n",
       " 'top': 259,\n",
       " 'involv': 260,\n",
       " 'piec': 261,\n",
       " 'includ': 262,\n",
       " 'fight': 263,\n",
       " 'viewer': 264,\n",
       " 'stupid': 265,\n",
       " 'entir': 266,\n",
       " 'releas': 267,\n",
       " 'fall': 268,\n",
       " 'home': 269,\n",
       " 'terribl': 270,\n",
       " 'simpli': 271,\n",
       " 'poor': 272,\n",
       " 'pictur': 273,\n",
       " 'power': 274,\n",
       " 'men': 275,\n",
       " 'coupl': 276,\n",
       " 'attempt': 277,\n",
       " 'camera': 278,\n",
       " 'usual': 279,\n",
       " 'produc': 280,\n",
       " 'less': 281,\n",
       " 'along': 282,\n",
       " 'featur': 283,\n",
       " 'video': 284,\n",
       " 'song': 285,\n",
       " 'portray': 286,\n",
       " 'disappoint': 287,\n",
       " 'low': 288,\n",
       " 'word': 289,\n",
       " 'possibl': 290,\n",
       " 'aw': 291,\n",
       " 'dead': 292,\n",
       " 'hollywood': 293,\n",
       " 'suppos': 294,\n",
       " 'school': 295,\n",
       " 'wrong': 296,\n",
       " 'absolut': 297,\n",
       " 'either': 298,\n",
       " 'lack': 299,\n",
       " 'definit': 300,\n",
       " 'except': 301,\n",
       " 'talent': 302,\n",
       " 'titl': 303,\n",
       " 'women': 304,\n",
       " 'given': 305,\n",
       " 'rest': 306,\n",
       " 'full': 307,\n",
       " 'emot': 308,\n",
       " 'perfect': 309,\n",
       " 'next': 310,\n",
       " 'game': 311,\n",
       " 'writer': 312,\n",
       " 'decid': 313,\n",
       " 'sex': 314,\n",
       " 'style': 315,\n",
       " 'truli': 316,\n",
       " 'save': 317,\n",
       " 'mr': 318,\n",
       " 'close': 319,\n",
       " 'sort': 320,\n",
       " 'comment': 321,\n",
       " 'mother': 322,\n",
       " 'sever': 323,\n",
       " 'case': 324,\n",
       " 'bring': 325,\n",
       " 'came': 326,\n",
       " 'joke': 327,\n",
       " 'dialogu': 328,\n",
       " 'flick': 329,\n",
       " 'humor': 330,\n",
       " 'perhap': 331,\n",
       " 'creat': 332,\n",
       " 'heart': 333,\n",
       " 'base': 334,\n",
       " 'age': 335,\n",
       " 'art': 336,\n",
       " 'dark': 337,\n",
       " 'small': 338,\n",
       " 'guess': 339,\n",
       " 'other': 340,\n",
       " 'stop': 341,\n",
       " 'meet': 342,\n",
       " 'sequenc': 343,\n",
       " 'experi': 344,\n",
       " 'earli': 345,\n",
       " 'often': 346,\n",
       " 'drama': 347,\n",
       " 'written': 348,\n",
       " 'forc': 349,\n",
       " 'children': 350,\n",
       " 'brother': 351,\n",
       " 'killer': 352,\n",
       " 'light': 353,\n",
       " 'mention': 354,\n",
       " 'consid': 355,\n",
       " 'actress': 356,\n",
       " 'cinema': 357,\n",
       " 'ye': 358,\n",
       " 'exampl': 359,\n",
       " 'side': 360,\n",
       " 'qualiti': 361,\n",
       " 'cut': 362,\n",
       " 'unfortun': 363,\n",
       " 'develop': 364,\n",
       " 'went': 365,\n",
       " 'amaz': 366,\n",
       " 'manag': 367,\n",
       " 'imagin': 368,\n",
       " 'oh': 369,\n",
       " 'horribl': 370,\n",
       " 'certainli': 371,\n",
       " 'lost': 372,\n",
       " 'car': 373,\n",
       " 'extrem': 374,\n",
       " 'wors': 375,\n",
       " 'matter': 376,\n",
       " 'white': 377,\n",
       " 'hit': 378,\n",
       " 'ask': 379,\n",
       " 'present': 380,\n",
       " 'voic': 381,\n",
       " 'fail': 382,\n",
       " 'overal': 383,\n",
       " 'impress': 384,\n",
       " 'natur': 385,\n",
       " 'felt': 386,\n",
       " 'son': 387,\n",
       " 'stand': 388,\n",
       " 'type': 389,\n",
       " 'basic': 390,\n",
       " 'favorit': 391,\n",
       " 'hero': 392,\n",
       " 'support': 393,\n",
       " 'evil': 394,\n",
       " 'wait': 395,\n",
       " 'learn': 396,\n",
       " 'despit': 397,\n",
       " 'alreadi': 398,\n",
       " 'danc': 399,\n",
       " 'throughout': 400,\n",
       " 'walk': 401,\n",
       " 'relationship': 402,\n",
       " 'number': 403,\n",
       " 'b': 404,\n",
       " 'genr': 405,\n",
       " 'question': 406,\n",
       " 'deal': 407,\n",
       " 'abl': 408,\n",
       " 'fine': 409,\n",
       " 'michael': 410,\n",
       " 'hate': 411,\n",
       " 'histori': 412,\n",
       " 'mysteri': 413,\n",
       " 'town': 414,\n",
       " 'rent': 415,\n",
       " 'success': 416,\n",
       " 'god': 417,\n",
       " 'today': 418,\n",
       " 'daughter': 419,\n",
       " 'credit': 420,\n",
       " 'wish': 421,\n",
       " 'late': 422,\n",
       " 'twist': 423,\n",
       " 'past': 424,\n",
       " 'realiz': 425,\n",
       " 'sit': 426,\n",
       " 'citi': 427,\n",
       " 'group': 428,\n",
       " 'score': 429,\n",
       " 'theme': 430,\n",
       " 'anyway': 431,\n",
       " 'deserv': 432,\n",
       " 'child': 433,\n",
       " 'pleas': 434,\n",
       " 'sometim': 435,\n",
       " 'edit': 436,\n",
       " 'stay': 437,\n",
       " 'touch': 438,\n",
       " 'event': 439,\n",
       " 'etc': 440,\n",
       " 'situat': 441,\n",
       " 'annoy': 442,\n",
       " 'stuff': 443,\n",
       " 'brilliant': 444,\n",
       " 'level': 445,\n",
       " 'behind': 446,\n",
       " 'robert': 447,\n",
       " 'gave': 448,\n",
       " 'appar': 449,\n",
       " 'bodi': 450,\n",
       " 'hilari': 451,\n",
       " 'add': 452,\n",
       " 'major': 453,\n",
       " 'chanc': 454,\n",
       " 'blood': 455,\n",
       " 'soon': 456,\n",
       " 'incred': 457,\n",
       " 'obvious': 458,\n",
       " 'order': 459,\n",
       " 'slow': 460,\n",
       " 'return': 461,\n",
       " 'decent': 462,\n",
       " 'pace': 463,\n",
       " 'comic': 464,\n",
       " 'ridicul': 465,\n",
       " 'self': 466,\n",
       " 'speak': 467,\n",
       " 'highli': 468,\n",
       " 'continu': 469,\n",
       " 'figur': 470,\n",
       " 'import': 471,\n",
       " 'shoot': 472,\n",
       " 'known': 473,\n",
       " 'thank': 474,\n",
       " 'took': 475,\n",
       " 'heard': 476,\n",
       " 'element': 477,\n",
       " 'monster': 478,\n",
       " 'countri': 479,\n",
       " 'career': 480,\n",
       " 'sad': 481,\n",
       " 'pick': 482,\n",
       " 'documentari': 483,\n",
       " 'strang': 484,\n",
       " 'clich': 485,\n",
       " 'happi': 486,\n",
       " 'novel': 487,\n",
       " 'critic': 488,\n",
       " 'polic': 489,\n",
       " 'pain': 490,\n",
       " 'husband': 491,\n",
       " 'suspens': 492,\n",
       " 'predict': 493,\n",
       " 'told': 494,\n",
       " 'violenc': 495,\n",
       " 'break': 496,\n",
       " 'cool': 497,\n",
       " 'dream': 498,\n",
       " 'realiti': 499,\n",
       " 'strong': 500,\n",
       " 'recent': 501,\n",
       " 'state': 502,\n",
       " 'compar': 503,\n",
       " 'crap': 504,\n",
       " 'ok': 505,\n",
       " 'hold': 506,\n",
       " 'caus': 507,\n",
       " 'visual': 508,\n",
       " 'hell': 509,\n",
       " 'effort': 510,\n",
       " 'opinion': 511,\n",
       " 'particularli': 512,\n",
       " 'sister': 513,\n",
       " 'serious': 514,\n",
       " 'zombi': 515,\n",
       " 'offer': 516,\n",
       " 'david': 517,\n",
       " 'gore': 518,\n",
       " 'simpl': 519,\n",
       " 'sequel': 520,\n",
       " 'provid': 521,\n",
       " 'room': 522,\n",
       " 'result': 523,\n",
       " 'obviou': 524,\n",
       " 'deliv': 525,\n",
       " 'jame': 526,\n",
       " 'seriou': 527,\n",
       " 'ladi': 528,\n",
       " 'huge': 529,\n",
       " 'theater': 530,\n",
       " 'explain': 531,\n",
       " 'local': 532,\n",
       " 'ago': 533,\n",
       " 'femal': 534,\n",
       " 'thriller': 535,\n",
       " 'shown': 536,\n",
       " 'taken': 537,\n",
       " 'cop': 538,\n",
       " 'english': 539,\n",
       " 'allow': 540,\n",
       " 'valu': 541,\n",
       " 'hear': 542,\n",
       " 'buy': 543,\n",
       " 'check': 544,\n",
       " 'dog': 545,\n",
       " 'exist': 546,\n",
       " 'note': 547,\n",
       " 'subject': 548,\n",
       " 'silli': 549,\n",
       " 'alon': 550,\n",
       " 'across': 551,\n",
       " 'spoiler': 552,\n",
       " 'none': 553,\n",
       " 'shock': 554,\n",
       " 'offic': 555,\n",
       " 'team': 556,\n",
       " 'exactli': 557,\n",
       " 'class': 558,\n",
       " 'rock': 559,\n",
       " 'dialog': 560,\n",
       " 'cinematographi': 561,\n",
       " 'confus': 562,\n",
       " 'convinc': 563,\n",
       " 'avoid': 564,\n",
       " 'sexual': 565,\n",
       " 'polit': 566,\n",
       " 'cover': 567,\n",
       " 'similar': 568,\n",
       " 'whose': 569,\n",
       " 'jack': 570,\n",
       " 'middl': 571,\n",
       " 'season': 572,\n",
       " 'apart': 573,\n",
       " 'excit': 574,\n",
       " 'parent': 575,\n",
       " 'build': 576,\n",
       " 'win': 577,\n",
       " 'form': 578,\n",
       " 'four': 579,\n",
       " 'modern': 580,\n",
       " 'messag': 581,\n",
       " 'scari': 582,\n",
       " 'pull': 583,\n",
       " 'tale': 584,\n",
       " 'filmmak': 585,\n",
       " 'carri': 586,\n",
       " 'mostli': 587,\n",
       " 'attent': 588,\n",
       " 'beyond': 589,\n",
       " 'televis': 590,\n",
       " 'singl': 591,\n",
       " 'escap': 592,\n",
       " 'somewhat': 593,\n",
       " 'pass': 594,\n",
       " 'street': 595,\n",
       " 'pay': 596,\n",
       " 'knew': 597,\n",
       " 'cheap': 598,\n",
       " 'king': 599,\n",
       " 'drug': 600,\n",
       " 'train': 601,\n",
       " 'fill': 602,\n",
       " 'detail': 603,\n",
       " 'non': 604,\n",
       " 'unlik': 605,\n",
       " 'due': 606,\n",
       " 'upon': 607,\n",
       " 'western': 608,\n",
       " 'imag': 609,\n",
       " 'typic': 610,\n",
       " 'respect': 611,\n",
       " 'remind': 612,\n",
       " 'william': 613,\n",
       " 'member': 614,\n",
       " 'earth': 615,\n",
       " 'villain': 616,\n",
       " 'charm': 617,\n",
       " 'toward': 618,\n",
       " 'oscar': 619,\n",
       " 'gun': 620,\n",
       " 'space': 621,\n",
       " 'crime': 622,\n",
       " 'georg': 623,\n",
       " 'doubt': 624,\n",
       " 'sing': 625,\n",
       " 'prove': 626,\n",
       " 'fit': 627,\n",
       " 'remain': 628,\n",
       " 'fire': 629,\n",
       " 'clearli': 630,\n",
       " 'stage': 631,\n",
       " 'drive': 632,\n",
       " 'ten': 633,\n",
       " 'air': 634,\n",
       " 'five': 635,\n",
       " 'intellig': 636,\n",
       " 'british': 637,\n",
       " 'discov': 638,\n",
       " 'atmospher': 639,\n",
       " 'marri': 640,\n",
       " 'french': 641,\n",
       " 'plan': 642,\n",
       " 'weak': 643,\n",
       " 'alien': 644,\n",
       " 'artist': 645,\n",
       " 'fast': 646,\n",
       " 'straight': 647,\n",
       " 'romant': 648,\n",
       " 'soundtrack': 649,\n",
       " 'near': 650,\n",
       " 'captur': 651,\n",
       " 'futur': 652,\n",
       " 'realist': 653,\n",
       " 'date': 654,\n",
       " 'appreci': 655,\n",
       " 'clear': 656,\n",
       " 'whether': 657,\n",
       " 'posit': 658,\n",
       " 'box': 659,\n",
       " 'spend': 660,\n",
       " 'notic': 661,\n",
       " 'teenag': 662,\n",
       " 'forget': 663,\n",
       " 'peter': 664,\n",
       " 'mark': 665,\n",
       " 'ad': 666,\n",
       " 'richard': 667,\n",
       " 'easili': 668,\n",
       " 'attack': 669,\n",
       " 'dull': 670,\n",
       " 'chase': 671,\n",
       " 'aspect': 672,\n",
       " 'fantast': 673,\n",
       " 'soldier': 674,\n",
       " 'doctor': 675,\n",
       " 'adult': 676,\n",
       " 'within': 677,\n",
       " 'copi': 678,\n",
       " 'battl': 679,\n",
       " 'e': 680,\n",
       " 'bunch': 681,\n",
       " 'period': 682,\n",
       " 'victim': 683,\n",
       " 'materi': 684,\n",
       " 'easi': 685,\n",
       " 'accept': 686,\n",
       " 'storylin': 687,\n",
       " 'suck': 688,\n",
       " 'agre': 689,\n",
       " 'adapt': 690,\n",
       " 'sorri': 691,\n",
       " 'inspir': 692,\n",
       " 'master': 693,\n",
       " 'certain': 694,\n",
       " 'bill': 695,\n",
       " 'water': 696,\n",
       " 'locat': 697,\n",
       " 'truth': 698,\n",
       " 'standard': 699,\n",
       " 'color': 700,\n",
       " 'larg': 701,\n",
       " 'among': 702,\n",
       " 'list': 703,\n",
       " 'student': 704,\n",
       " 'gone': 705,\n",
       " 'suggest': 706,\n",
       " 'busi': 707,\n",
       " 'dr': 708,\n",
       " 'eventu': 709,\n",
       " 'mess': 710,\n",
       " 'lose': 711,\n",
       " 'match': 712,\n",
       " 'finish': 713,\n",
       " 'th': 714,\n",
       " 'issu': 715,\n",
       " 'cartoon': 716,\n",
       " 'contain': 717,\n",
       " 'cri': 718,\n",
       " 'okay': 719,\n",
       " 'suffer': 720,\n",
       " 'throw': 721,\n",
       " 'cultur': 722,\n",
       " 'reveal': 723,\n",
       " 'red': 724,\n",
       " 'kept': 725,\n",
       " 'nearli': 726,\n",
       " 'troubl': 727,\n",
       " 'shame': 728,\n",
       " 'vampir': 729,\n",
       " 'greatest': 730,\n",
       " 'famou': 731,\n",
       " 'adventur': 732,\n",
       " 'paul': 733,\n",
       " 'dramat': 734,\n",
       " 'normal': 735,\n",
       " 'romanc': 736,\n",
       " 'wit': 737,\n",
       " 'premis': 738,\n",
       " 'tom': 739,\n",
       " 'attract': 740,\n",
       " 'describ': 741,\n",
       " 'ultim': 742,\n",
       " 'mix': 743,\n",
       " 'brought': 744,\n",
       " 'relat': 745,\n",
       " 'somehow': 746,\n",
       " 'refer': 747,\n",
       " 'america': 748,\n",
       " 'masterpiec': 749,\n",
       " 'male': 750,\n",
       " 'rare': 751,\n",
       " 'averag': 752,\n",
       " 'imdb': 753,\n",
       " 'memor': 754,\n",
       " 'third': 755,\n",
       " 'prison': 756,\n",
       " 'express': 757,\n",
       " 'parti': 758,\n",
       " 'de': 759,\n",
       " 'bother': 760,\n",
       " 'secret': 761,\n",
       " 'background': 762,\n",
       " 'german': 763,\n",
       " 'gay': 764,\n",
       " 'pure': 765,\n",
       " 'societi': 766,\n",
       " 'deep': 767,\n",
       " 'lame': 768,\n",
       " 'fear': 769,\n",
       " 'york': 770,\n",
       " 'spirit': 771,\n",
       " 'babi': 772,\n",
       " 'forward': 773,\n",
       " 'hot': 774,\n",
       " 'fi': 775,\n",
       " 'free': 776,\n",
       " 'sci': 777,\n",
       " 'project': 778,\n",
       " 'whatev': 779,\n",
       " 'warn': 780,\n",
       " 'treat': 781,\n",
       " 'particular': 782,\n",
       " 'admit': 783,\n",
       " 'island': 784,\n",
       " 'crew': 785,\n",
       " 'inde': 786,\n",
       " 'poorli': 787,\n",
       " 'scare': 788,\n",
       " 'studio': 789,\n",
       " 'choic': 790,\n",
       " 'wood': 791,\n",
       " 'appeal': 792,\n",
       " 'odd': 793,\n",
       " 'control': 794,\n",
       " 'fiction': 795,\n",
       " 'accent': 796,\n",
       " 'lie': 797,\n",
       " 'cheesi': 798,\n",
       " 'week': 799,\n",
       " 'mad': 800,\n",
       " 'fli': 801,\n",
       " 'screenplay': 802,\n",
       " 'lee': 803,\n",
       " 'design': 804,\n",
       " 'wear': 805,\n",
       " 'weird': 806,\n",
       " 'difficult': 807,\n",
       " 'amus': 808,\n",
       " 'girlfriend': 809,\n",
       " 'depict': 810,\n",
       " 'footag': 811,\n",
       " 'dumb': 812,\n",
       " 'unless': 813,\n",
       " 'crazi': 814,\n",
       " 'uniqu': 815,\n",
       " 'lover': 816,\n",
       " 'award': 817,\n",
       " 'becam': 818,\n",
       " 'struggl': 819,\n",
       " 'otherwis': 820,\n",
       " 'costum': 821,\n",
       " 'insid': 822,\n",
       " 'flaw': 823,\n",
       " 'grow': 824,\n",
       " 'potenti': 825,\n",
       " 'public': 826,\n",
       " 'plu': 827,\n",
       " 'stereotyp': 828,\n",
       " 'ride': 829,\n",
       " 'remak': 830,\n",
       " 'outsid': 831,\n",
       " 'connect': 832,\n",
       " 'superb': 833,\n",
       " 'fantasi': 834,\n",
       " 'scream': 835,\n",
       " 'moral': 836,\n",
       " 'rich': 837,\n",
       " 'brain': 838,\n",
       " 'perfectli': 839,\n",
       " 'magic': 840,\n",
       " 'surviv': 841,\n",
       " 'badli': 842,\n",
       " 'japanes': 843,\n",
       " 'land': 844,\n",
       " 'previou': 845,\n",
       " 'stick': 846,\n",
       " 'creepi': 847,\n",
       " 'execut': 848,\n",
       " 'maker': 849,\n",
       " 'quickli': 850,\n",
       " 'bare': 851,\n",
       " 'roll': 852,\n",
       " 'harri': 853,\n",
       " 'plenti': 854,\n",
       " 'front': 855,\n",
       " 'disney': 856,\n",
       " 'teen': 857,\n",
       " 'joe': 858,\n",
       " 'mari': 859,\n",
       " 'steal': 860,\n",
       " 'compani': 861,\n",
       " 'eat': 862,\n",
       " 'jump': 863,\n",
       " 'popular': 864,\n",
       " 'dress': 865,\n",
       " 'track': 866,\n",
       " 'earlier': 867,\n",
       " 'post': 868,\n",
       " 'beat': 869,\n",
       " 'memori': 870,\n",
       " 'term': 871,\n",
       " 'catch': 872,\n",
       " 'serv': 873,\n",
       " 'inform': 874,\n",
       " 'amount': 875,\n",
       " 'cat': 876,\n",
       " 'answer': 877,\n",
       " 'disturb': 878,\n",
       " 'c': 879,\n",
       " 'promis': 880,\n",
       " 'older': 881,\n",
       " 'travel': 882,\n",
       " 'concept': 883,\n",
       " 'combin': 884,\n",
       " 'listen': 885,\n",
       " 'co': 886,\n",
       " 'innoc': 887,\n",
       " 'band': 888,\n",
       " 'hair': 889,\n",
       " 'equal': 890,\n",
       " 'variou': 891,\n",
       " 'era': 892,\n",
       " 'store': 893,\n",
       " 'rip': 894,\n",
       " 'record': 895,\n",
       " 'italian': 896,\n",
       " 'law': 897,\n",
       " 'univers': 898,\n",
       " 'clever': 899,\n",
       " 'centuri': 900,\n",
       " 'fairli': 901,\n",
       " 'blue': 902,\n",
       " 'la': 903,\n",
       " 'histor': 904,\n",
       " 'spent': 905,\n",
       " 'cute': 906,\n",
       " 'channel': 907,\n",
       " 'kick': 908,\n",
       " 'concern': 909,\n",
       " 'creatur': 910,\n",
       " 'social': 911,\n",
       " 'plain': 912,\n",
       " 'danger': 913,\n",
       " 'intent': 914,\n",
       " 'soul': 915,\n",
       " 'mistak': 916,\n",
       " 'meant': 917,\n",
       " 'tast': 918,\n",
       " 'hole': 919,\n",
       " 'player': 920,\n",
       " 'purpos': 921,\n",
       " 'destroy': 922,\n",
       " 'abil': 923,\n",
       " 'collect': 924,\n",
       " 'languag': 925,\n",
       " 'familiar': 926,\n",
       " 'ghost': 927,\n",
       " 'intens': 928,\n",
       " 'caught': 929,\n",
       " 'depth': 930,\n",
       " 'sweet': 931,\n",
       " 'flat': 932,\n",
       " 'step': 933,\n",
       " 'hardli': 934,\n",
       " 'door': 935,\n",
       " 'engag': 936,\n",
       " 'camp': 937,\n",
       " 'ann': 938,\n",
       " 'share': 939,\n",
       " 'nuditi': 940,\n",
       " 'ignor': 941,\n",
       " 'tension': 942,\n",
       " 'spot': 943,\n",
       " 'extra': 944,\n",
       " 'park': 945,\n",
       " 'respons': 946,\n",
       " 'indian': 947,\n",
       " 'cold': 948,\n",
       " 'achiev': 949,\n",
       " 'fake': 950,\n",
       " 'liter': 951,\n",
       " 'introduc': 952,\n",
       " 'wrote': 953,\n",
       " 'pop': 954,\n",
       " 'fascin': 955,\n",
       " 'claim': 956,\n",
       " 'van': 957,\n",
       " 'desper': 958,\n",
       " 'scott': 959,\n",
       " 'limit': 960,\n",
       " 'violent': 961,\n",
       " 'commun': 962,\n",
       " 'reach': 963,\n",
       " 'scienc': 964,\n",
       " 'race': 965,\n",
       " 'search': 966,\n",
       " 'suit': 967,\n",
       " 'tone': 968,\n",
       " 'sleep': 969,\n",
       " 'drag': 970,\n",
       " 'arriv': 971,\n",
       " 'manner': 972,\n",
       " 'intrigu': 973,\n",
       " 'sadli': 974,\n",
       " 'fashion': 975,\n",
       " 'immedi': 976,\n",
       " 'trash': 977,\n",
       " 'complex': 978,\n",
       " 'million': 979,\n",
       " 'cross': 980,\n",
       " 'gang': 981,\n",
       " 'sick': 982,\n",
       " 'slightli': 983,\n",
       " 'tortur': 984,\n",
       " 'approach': 985,\n",
       " 'judg': 986,\n",
       " 'ruin': 987,\n",
       " 'nation': 988,\n",
       " 'unbeliev': 989,\n",
       " 'skill': 990,\n",
       " 'delight': 991,\n",
       " 'trip': 992,\n",
       " 'jane': 993,\n",
       " 'anti': 994,\n",
       " 'pathet': 995,\n",
       " 'colleg': 996,\n",
       " 'physic': 997,\n",
       " 'burn': 998,\n",
       " 'bizarr': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['br', 'movi', 'film', 'one', 'like', 'time', 'good', 'make', 'charact', 'see']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.wv.index_to_key[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12050498\n"
     ]
    }
   ],
   "source": [
    "print(cbow.wv.similarity(\"make\",\"build\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('decent', 0.7544897198677063), ('great', 0.7220576405525208), ('bad', 0.702559769153595), ('alright', 0.6370170712471008), ('okay', 0.6304665207862854), ('ok', 0.6071308851242065), ('nice', 0.5829575657844543), ('darn', 0.5818901658058167), ('cool', 0.5749438405036926), ('excel', 0.5693867802619934)]\n"
     ]
    }
   ],
   "source": [
    "print(cbow.wv.similar_by_key(\"good\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19329"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbow.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080357544"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me = np.array(cbow.wv.get_vector(\"one\"))\n",
    "me.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    \n",
    "    # doc1 contains those words of the document which are included in the vocab\n",
    "    doc1 = [word for word in doc.split() if word in cbow.wv.index_to_key]\n",
    "    \n",
    "    wv1 = []  # this will contain the WE of all the vocab words from the doc\n",
    "    for word in doc1:\n",
    "        wv1.append(cbow.wv.get_vector(word))\n",
    "    wv1_ = np.array(wv1)\n",
    "    wv1_mean = wv1_.mean(axis=0)\n",
    "    return wv1_mean\n",
    "\n",
    "# np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_review = stem_df[\"stemmed_corpus\"].apply(document_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.ones((len(temp_review), 300))*np.nan).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Combining all the document vectors into a singl numpy array (tweets_vec)\n",
    "embedding_size = 100\n",
    "review_vec = np.ones((len(temp_review), embedding_size))*np.nan\n",
    "for i in range(review_vec.shape[0]):\n",
    "    review_vec[i,:] = temp_review.iloc[i]\n",
    "\n",
    "review_vec.shape # this itself is your final FEATURE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005928780431859195"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_vec[0,:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# Create a new DF to store these new documnent features\n",
    "vec_df = pd.DataFrame(review_vec)\n",
    "vec_df['y'] = label\n",
    "vec_df.dropna(how='any', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.511467</td>\n",
       "      <td>0.176962</td>\n",
       "      <td>-0.113300</td>\n",
       "      <td>0.320372</td>\n",
       "      <td>-0.216529</td>\n",
       "      <td>-0.156066</td>\n",
       "      <td>0.435461</td>\n",
       "      <td>-0.119201</td>\n",
       "      <td>-0.264528</td>\n",
       "      <td>0.192353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167461</td>\n",
       "      <td>-0.082759</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.126074</td>\n",
       "      <td>-0.100811</td>\n",
       "      <td>0.050716</td>\n",
       "      <td>0.077237</td>\n",
       "      <td>0.210537</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.399278</td>\n",
       "      <td>0.047006</td>\n",
       "      <td>0.052484</td>\n",
       "      <td>0.279662</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>0.155836</td>\n",
       "      <td>0.251642</td>\n",
       "      <td>-0.128293</td>\n",
       "      <td>-0.344460</td>\n",
       "      <td>-0.034714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079432</td>\n",
       "      <td>-0.112114</td>\n",
       "      <td>-0.160047</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.310706</td>\n",
       "      <td>0.120169</td>\n",
       "      <td>0.061814</td>\n",
       "      <td>-0.232976</td>\n",
       "      <td>0.217732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.361898</td>\n",
       "      <td>0.038340</td>\n",
       "      <td>-0.284491</td>\n",
       "      <td>0.358288</td>\n",
       "      <td>0.006961</td>\n",
       "      <td>-0.175093</td>\n",
       "      <td>0.347266</td>\n",
       "      <td>-0.297755</td>\n",
       "      <td>-0.558613</td>\n",
       "      <td>-0.015828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129093</td>\n",
       "      <td>-0.140910</td>\n",
       "      <td>-0.151156</td>\n",
       "      <td>0.145863</td>\n",
       "      <td>-0.139230</td>\n",
       "      <td>0.067921</td>\n",
       "      <td>0.185767</td>\n",
       "      <td>0.114328</td>\n",
       "      <td>0.349683</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.481012</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>-0.358875</td>\n",
       "      <td>0.300214</td>\n",
       "      <td>-0.440127</td>\n",
       "      <td>-0.212287</td>\n",
       "      <td>0.616870</td>\n",
       "      <td>-0.241234</td>\n",
       "      <td>-0.464183</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199201</td>\n",
       "      <td>-0.083379</td>\n",
       "      <td>-0.252036</td>\n",
       "      <td>-0.132226</td>\n",
       "      <td>0.075301</td>\n",
       "      <td>0.072504</td>\n",
       "      <td>-0.029489</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.174011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.661942</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>-0.420661</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>-0.079016</td>\n",
       "      <td>-0.120627</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>-0.321117</td>\n",
       "      <td>-0.575652</td>\n",
       "      <td>0.069736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087338</td>\n",
       "      <td>-0.171173</td>\n",
       "      <td>0.052766</td>\n",
       "      <td>0.088402</td>\n",
       "      <td>0.186092</td>\n",
       "      <td>0.055097</td>\n",
       "      <td>0.132362</td>\n",
       "      <td>-0.241924</td>\n",
       "      <td>0.066894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.511467  0.176962 -0.113300  0.320372 -0.216529 -0.156066  0.435461   \n",
       "1  0.399278  0.047006  0.052484  0.279662  0.026666  0.155836  0.251642   \n",
       "2  0.361898  0.038340 -0.284491  0.358288  0.006961 -0.175093  0.347266   \n",
       "3  0.481012  0.009386 -0.358875  0.300214 -0.440127 -0.212287  0.616870   \n",
       "4  0.661942  0.012906 -0.420661  0.227609 -0.079016 -0.120627  0.232194   \n",
       "\n",
       "          7         8         9  ...        91        92        93        94  \\\n",
       "0 -0.119201 -0.264528  0.192353  ... -0.167461 -0.082759  0.082263  0.126074   \n",
       "1 -0.128293 -0.344460 -0.034714  ...  0.079432 -0.112114 -0.160047  0.017236   \n",
       "2 -0.297755 -0.558613 -0.015828  ...  0.129093 -0.140910 -0.151156  0.145863   \n",
       "3 -0.241234 -0.464183  0.280600  ...  0.199201 -0.083379 -0.252036 -0.132226   \n",
       "4 -0.321117 -0.575652  0.069736  ...  0.087338 -0.171173  0.052766  0.088402   \n",
       "\n",
       "         95        96        97        98        99  y  \n",
       "0 -0.100811  0.050716  0.077237  0.210537  0.006237  1  \n",
       "1  0.310706  0.120169  0.061814 -0.232976  0.217732  1  \n",
       "2 -0.139230  0.067921  0.185767  0.114328  0.349683  1  \n",
       "3  0.075301  0.072504 -0.029489  0.219512  0.174011  0  \n",
       "4  0.186092  0.055097  0.132362 -0.241924  0.066894  1  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 101)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.511467</td>\n",
       "      <td>0.176962</td>\n",
       "      <td>-0.113300</td>\n",
       "      <td>0.320372</td>\n",
       "      <td>-0.216529</td>\n",
       "      <td>-0.156066</td>\n",
       "      <td>0.435461</td>\n",
       "      <td>-0.119201</td>\n",
       "      <td>-0.264528</td>\n",
       "      <td>0.192353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292195</td>\n",
       "      <td>-0.167461</td>\n",
       "      <td>-0.082759</td>\n",
       "      <td>0.082263</td>\n",
       "      <td>0.126074</td>\n",
       "      <td>-0.100811</td>\n",
       "      <td>0.050716</td>\n",
       "      <td>0.077237</td>\n",
       "      <td>0.210537</td>\n",
       "      <td>0.006237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.399278</td>\n",
       "      <td>0.047006</td>\n",
       "      <td>0.052484</td>\n",
       "      <td>0.279662</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>0.155836</td>\n",
       "      <td>0.251642</td>\n",
       "      <td>-0.128293</td>\n",
       "      <td>-0.344460</td>\n",
       "      <td>-0.034714</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012104</td>\n",
       "      <td>0.079432</td>\n",
       "      <td>-0.112114</td>\n",
       "      <td>-0.160047</td>\n",
       "      <td>0.017236</td>\n",
       "      <td>0.310706</td>\n",
       "      <td>0.120169</td>\n",
       "      <td>0.061814</td>\n",
       "      <td>-0.232976</td>\n",
       "      <td>0.217732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.361898</td>\n",
       "      <td>0.038340</td>\n",
       "      <td>-0.284491</td>\n",
       "      <td>0.358288</td>\n",
       "      <td>0.006961</td>\n",
       "      <td>-0.175093</td>\n",
       "      <td>0.347266</td>\n",
       "      <td>-0.297755</td>\n",
       "      <td>-0.558613</td>\n",
       "      <td>-0.015828</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209395</td>\n",
       "      <td>0.129093</td>\n",
       "      <td>-0.140910</td>\n",
       "      <td>-0.151156</td>\n",
       "      <td>0.145863</td>\n",
       "      <td>-0.139230</td>\n",
       "      <td>0.067921</td>\n",
       "      <td>0.185767</td>\n",
       "      <td>0.114328</td>\n",
       "      <td>0.349683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.481012</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>-0.358875</td>\n",
       "      <td>0.300214</td>\n",
       "      <td>-0.440127</td>\n",
       "      <td>-0.212287</td>\n",
       "      <td>0.616870</td>\n",
       "      <td>-0.241234</td>\n",
       "      <td>-0.464183</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.609337</td>\n",
       "      <td>0.199201</td>\n",
       "      <td>-0.083379</td>\n",
       "      <td>-0.252036</td>\n",
       "      <td>-0.132226</td>\n",
       "      <td>0.075301</td>\n",
       "      <td>0.072504</td>\n",
       "      <td>-0.029489</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.174011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.661942</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>-0.420661</td>\n",
       "      <td>0.227609</td>\n",
       "      <td>-0.079016</td>\n",
       "      <td>-0.120627</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>-0.321117</td>\n",
       "      <td>-0.575652</td>\n",
       "      <td>0.069736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217747</td>\n",
       "      <td>0.087338</td>\n",
       "      <td>-0.171173</td>\n",
       "      <td>0.052766</td>\n",
       "      <td>0.088402</td>\n",
       "      <td>0.186092</td>\n",
       "      <td>0.055097</td>\n",
       "      <td>0.132362</td>\n",
       "      <td>-0.241924</td>\n",
       "      <td>0.066894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.511467  0.176962 -0.113300  0.320372 -0.216529 -0.156066  0.435461   \n",
       "1  0.399278  0.047006  0.052484  0.279662  0.026666  0.155836  0.251642   \n",
       "2  0.361898  0.038340 -0.284491  0.358288  0.006961 -0.175093  0.347266   \n",
       "3  0.481012  0.009386 -0.358875  0.300214 -0.440127 -0.212287  0.616870   \n",
       "4  0.661942  0.012906 -0.420661  0.227609 -0.079016 -0.120627  0.232194   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0 -0.119201 -0.264528  0.192353  ... -0.292195 -0.167461 -0.082759  0.082263   \n",
       "1 -0.128293 -0.344460 -0.034714  ... -0.012104  0.079432 -0.112114 -0.160047   \n",
       "2 -0.297755 -0.558613 -0.015828  ... -0.209395  0.129093 -0.140910 -0.151156   \n",
       "3 -0.241234 -0.464183  0.280600  ... -0.609337  0.199201 -0.083379 -0.252036   \n",
       "4 -0.321117 -0.575652  0.069736  ... -0.217747  0.087338 -0.171173  0.052766   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.126074 -0.100811  0.050716  0.077237  0.210537  0.006237  \n",
       "1  0.017236  0.310706  0.120169  0.061814 -0.232976  0.217732  \n",
       "2  0.145863 -0.139230  0.067921  0.185767  0.114328  0.349683  \n",
       "3 -0.132226  0.075301  0.072504 -0.029489  0.219512  0.174011  \n",
       "4  0.088402  0.186092  0.055097  0.132362 -0.241924  0.066894  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_word_emb = vec_df.drop('y', axis=1)\n",
    "y = vec_df['y']\n",
    "X_word_emb.shape\n",
    "X_word_emb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.09 0.07\n",
      "79.77 0.2\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4, random_state=42)\n",
    "WE_pipe = Pipeline([('SC', StandardScaler()), ('LR', LR1)] )\n",
    "\n",
    "results = cross_validate(WE_pipe, X_word_emb, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting teh data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37500, 100) (12500, 100) (37500,) (12500,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_word_emb, y, test_size=0.25, random_state=123)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdot/PycharmProjects/IMDB_Movie_talk/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5341  818]\n",
      " [ 887 5454]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8636\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86      6159\n",
      "           1       0.87      0.86      0.86      6341\n",
      "\n",
      "    accuracy                           0.86     12500\n",
      "   macro avg       0.86      0.86      0.86     12500\n",
      "weighted avg       0.86      0.86      0.86     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4887 1741]\n",
      " [1341 4531]]\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(confusion_matrix(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75344\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76      6628\n",
      "           1       0.72      0.77      0.75      5872\n",
      "\n",
      "    accuracy                           0.75     12500\n",
      "   macro avg       0.75      0.75      0.75     12500\n",
      "weighted avg       0.76      0.75      0.75     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5408 1836]\n",
      " [2061 5695]]\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7402\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.74      7244\n",
      "           1       0.76      0.73      0.75      7756\n",
      "\n",
      "    accuracy                           0.74     15000\n",
      "   macro avg       0.74      0.74      0.74     15000\n",
      "weighted avg       0.74      0.74      0.74     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4554 1742]\n",
      " [1674 4530]]\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train,y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "print(confusion_matrix(y_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72672\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.72      0.73      6296\n",
      "           1       0.72      0.73      0.73      6204\n",
      "\n",
      "    accuracy                           0.73     12500\n",
      "   macro avg       0.73      0.73      0.73     12500\n",
      "weighted avg       0.73      0.73      0.73     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5087  922]\n",
      " [1141 5350]]\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train,y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83496\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83      6009\n",
      "           1       0.85      0.82      0.84      6491\n",
      "\n",
      "    accuracy                           0.83     12500\n",
      "   macro avg       0.83      0.84      0.83     12500\n",
      "weighted avg       0.84      0.83      0.84     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight matrix for the embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen data Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unseen data processing for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = \"good movie\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'movi']\n"
     ]
    }
   ],
   "source": [
    "corpus1 = []\n",
    "review = word_tokenize(rw)\n",
    "    # print(review)\n",
    "    # print()\n",
    "review = [w.lower() for w in review if w.isalpha()]\n",
    "    # print(review)\n",
    "    # print()\n",
    "review = [ps.stem(w) for w in review if w not in stopwords.words('english')]\n",
    "    # print(review)\n",
    "    # print()\n",
    "# review = \" \".join(review)\n",
    "# corpus1.append(review)\n",
    "print(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vec_voc(sen):\n",
    "    embd = [cbow.wv[c] for c in sen if c in cbow.wv.index_to_key]\n",
    "    return sum(embd)/len(embd) if embd else None\n",
    "\n",
    "\n",
    "voc_vec = vec_voc(review)\n",
    "print(voc_vec.shape)\n",
    "reshape_voc_vec = voc_vec.reshape(1,-1)\n",
    "lr.predict(reshape_voc_vec)\n",
    "# reshape_voc_vec.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[393], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[43mcbow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m(corpus1)\n\u001b[1;32m      2\u001b[0m cbow\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m      3\u001b[0m cbow\u001b[38;5;241m.\u001b[39mvocabulary_\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "v = cbow.transform(corpus1)\n",
    "cbow.get_feature_names_out()\n",
    "cbow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = v.toarray()\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unseen preprocessed data for applying on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(reshape_voc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.predict(reshape_voc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.predict(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.predict(reshape_voc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.predict(reshape_voc_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
